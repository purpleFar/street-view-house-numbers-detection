{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "street_view_house_numbers_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wGv7zAiGtFa"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import subprocess\n",
        "print(subprocess.check_output(\"pip install mmcv-full==latest+torch{} -f https://download.openmmlab.com/mmcv/dist/index.html\".format(torch.__version__), shell=True))\n",
        "!git clone https://github.com/purpleFar/street-view-house-numbers-detection.git\n",
        "os.chdir('street-view-house-numbers-detection/')\n",
        "!pip install -r requirements/build.txt\n",
        "!pip install -v -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGqbyQ_IOTu4",
        "outputId": "98caaae9-826b-4b31-f25f-37a05a03f44d"
      },
      "source": [
        "!nvidia-smi #show gpu information"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Nov 26 18:32:40 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydtdge_Kpecc",
        "outputId": "7a634b30-0766-4cb7-84e9-84f583349dc0"
      },
      "source": [
        "import os\n",
        "from mmdet.apis import init_detector\n",
        "import numpy as np\n",
        "import torch\n",
        "from mmcv.ops import RoIPool\n",
        "from mmcv.parallel import collate, scatter\n",
        "\n",
        "from mmdet.datasets.pipelines import Compose\n",
        "import time\n",
        "\n",
        "total_time = 0\n",
        "test_dir = 'inference_demo'\n",
        "config_file = \"configs/faster_rcnn/faster_rcnn_r34_fpn_1x_coco.py\"\n",
        "model = init_detector(config_file, device=\"cuda:0\")\n",
        "\n",
        "def inference_detector(model, img):\n",
        "    cfg = model.cfg\n",
        "    device = next(model.parameters()).device  # model device\n",
        "    # prepare data\n",
        "    if isinstance(img, np.ndarray):\n",
        "        # directly add img\n",
        "        data = dict(img=img)\n",
        "        cfg = cfg.copy()\n",
        "        # set loading pipeline type\n",
        "        cfg.data.test.pipeline[0].type = 'LoadImageFromWebcam'\n",
        "    else:\n",
        "        # add information into dict\n",
        "        data = dict(img_info=dict(filename=img), img_prefix=None)\n",
        "    # build the data pipeline\n",
        "    test_pipeline = Compose(cfg.data.test.pipeline)\n",
        "    data = test_pipeline(data)\n",
        "    data = collate([data], samples_per_gpu=1)\n",
        "    if next(model.parameters()).is_cuda:\n",
        "        # scatter to specified GPU\n",
        "        data = scatter(data, [device])[0]\n",
        "    else:\n",
        "        for m in model.modules():\n",
        "            assert not isinstance(\n",
        "                m, RoIPool\n",
        "            ), 'CPU inference with RoIPool is not supported currently.'\n",
        "        # just get the actual data from DataContainer\n",
        "        data['img_metas'] = data['img_metas'][0].data\n",
        "\n",
        "    # forward the model\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        model(return_loss=False, rescale=True, **data)[0]\n",
        "        delay = time.time()-start\n",
        "    return delay\n",
        "\n",
        "for _, _, files in os.walk(test_dir):\n",
        "    num = len(files)\n",
        "    for file in files:\n",
        "        total_time += inference_detector(model, os.path.join(test_dir, file))\n",
        "        \n",
        "fps = num/total_time\n",
        "t_per_loop = int(total_time*1000/num)\n",
        "print('Run {} loops, use {:.4} seconds in model inference. (Not included images loading times)'.format(num,total_time))\n",
        "print('fps:{:.4}, {}ms per loop'.format(fps,t_per_loop))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run 50 loops, use 1.758 seconds in model inference. (Not included images loading times)\n",
            "fps:28.45, 35ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}